{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91438,"databundleVersionId":11038850,"sourceType":"competition"},{"sourceId":10689596,"sourceType":"datasetVersion","datasetId":6623231}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prepocess","metadata":{"execution":{"iopub.status.busy":"2025-02-07T15:45:24.184141Z","iopub.execute_input":"2025-02-07T15:45:24.184482Z","iopub.status.idle":"2025-02-07T15:45:24.198860Z","shell.execute_reply.started":"2025-02-07T15:45:24.184444Z","shell.execute_reply":"2025-02-07T15:45:24.197699Z"}}},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import pandas as pd\n# import multiprocessing  # For parallel processing\n\n# def process_npy_file(file_path):\n#     \"\"\"Processes a single .npy file and returns row and column counts.\"\"\"\n#     try:\n#         data = np.load(file_path, mmap_mode='r')  # Use memory mapping if possible\n#         rows = data.shape[0]\n#         cols = data.shape[1] if data.ndim > 1 else 1\n#         return rows, cols, os.path.basename(file_path) # Return filename as well\n#     except (OSError, ValueError) as e:\n#         print(f\"Error processing {file_path}: {e}\")\n#         return None, None, os.path.basename(file_path)  # Return None for failed files\n#     except Exception as e:\n#         print(f\"An unexpected error occurred processing {file_path}: {e}\")\n#         return None, None, os.path.basename(file_path)  # Return None for failed files\n\n\n# def find_Row_Col(path, num_processes=None):\n#     if not os.path.exists(path):\n#         print(f\"Error: Path '{path}' not found.\")\n#         return None\n\n#     npy_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".npy\")]\n\n#     if num_processes is None:\n#         num_processes = multiprocessing.cpu_count()  # Use all available cores\n\n#     pool = multiprocessing.Pool(processes=num_processes)  # Create a process pool\n#     results = pool.map(process_npy_file, npy_files)  # Process files in parallel\n#     pool.close()  # Close the pool\n#     pool.join()   # Wait for all processes to finish\n\n#     lst_row = []\n#     lst_column = []\n#     filenames = []\n\n#     for rows, cols, filename in results:\n#         if rows is not None:  # Check if processing was successful\n#             lst_row.append(rows)\n#             lst_column.append(cols)\n#             filenames.append(os.path.join(path,filename))\n\n#     df = pd.DataFrame({'Rows': lst_row, 'Columns': lst_column}, index=filenames)\n#     return df\n\n\n# # Example usage (adjust num_processes if needed):\n# train_path = \"/kaggle/input/signal-fast-radio-burst-detection/train/train\"\n# test_path = \"/kaggle/input/signal-fast-radio-burst-detection/test/test\"\n\n# train_df = find_Row_Col(train_path)\n# test_df = find_Row_Col(test_path)\n\n# if train_df is not None:\n#     print(\"Train DataFrame:\")\n#     print(train_df)\n#     train_df.to_csv(\"train_data.csv\")\n# else:\n#     print(\"Error: Train DataFrame could not be created.\")\n\n# if test_df is not None:\n#     print(\"\\nTest DataFrame:\")\n#     print(test_df)\n#     test_df.to_csv(\"test_data.csv\")\n# else:\n#     print(\"Error: Test DataFrame could not be created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Rows\")\n# print(train_df.Rows.unique())\n# print(test_df.Rows.unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Columns\")\n# print(train_df.Columns.unique())\n# print(test_df.Columns.unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"File_paths\")\n# print(train_df.index.unique())\n# print(test_df.index.unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import os\n# from PIL import Image\n# from tqdm import tqdm\n# import multiprocessing\n# from functools import partial\n\n# def process_single_array(array_data, file_base, array_index, output_dir):\n#     \"\"\"Processes a single array chunk.\"\"\"\n#     try:\n#         current_array = array_data\n\n#         if current_array.dtype != np.uint8:\n#             min_val = np.min(current_array)\n#             max_val = np.max(current_array)\n#             if max_val - min_val != 0:\n#                 current_array = ((current_array - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n#             else:\n#                 current_array = np.zeros_like(current_array, dtype=np.uint8)\n\n#         img = Image.fromarray(current_array)\n#         if img.mode != 'L':\n#             img = img.convert(\"L\")\n#         output_filename = f\"{file_base}_{array_index}.jpg\"\n#         output_path = os.path.join(output_dir, output_filename)\n#         img.save(output_path)\n#         return True  # Indicate success\n\n#     except Exception as e:\n#         print(f\"Error converting/saving {file_base}, part {array_index}: {e}\")\n#         import matplotlib.pyplot as plt\n#         plt.imshow(current_array)\n#         plt.title(\"Array before conversion\")\n#         plt.show()\n#         raise  # Re-raise for full traceback\n#         return False # Indicate failure\n\n\n# def process_file(file_path, output_dir):  # output_dir is now passed directly\n#     try:\n#         array_list = np.load(file_path)\n#         file_base = os.path.splitext(os.path.basename(file_path))[0]\n#         success_count = 0\n\n#         with tqdm(range(len(array_list) // 256), desc=f\"Processing {file_base}\", leave=False) as pbar:\n#             for array_index in pbar:\n#                 start_index = array_index * 256\n#                 end_index = min((array_index + 1) * 256, len(array_list))\n#                 current_array = array_list[start_index:end_index]\n\n#                 if process_single_array(current_array, file_base, array_index, output_dir):\n#                     success_count += 1\n#         return success_count\n\n#     except FileNotFoundError:\n#         print(f\"File not found: {file_path}\")\n#         return 0\n#     except Exception as e:\n#         print(f\"Error processing file {file_path}: {e}\")\n#         return 0\n\n\n# def process_data_multiprocessing(df, output_dir=\"output_images\", num_processes=None):\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n\n#     if num_processes is None:\n#         num_processes = multiprocessing.cpu_count()\n\n#     pool = multiprocessing.Pool(processes=num_processes)\n#     total_files = len(df.index)\n#     total_success = 0\n#     total_chunks = 0\n\n#     file_paths = list(df.index)  # Convert index to a list for consistent ordering\n\n#     with tqdm(total=total_files, desc=f\"Total Progress\") as pbar_total:\n#         for i, result in enumerate(pool.imap_unordered(partial(process_file, output_dir=output_dir), file_paths)):\n#             total_success += result\n#             pbar_total.update(1)\n\n#             # Calculate total chunks using the *original* file path order:\n#             file_path = file_paths[i]  # Use the correct index from the list\n#             try:\n#                 array_list_temp = np.load(file_path)\n#                 total_chunks += len(array_list_temp) // 256\n#             except Exception as e:\n#                 print(f\"Error loading {file_path}: {e}\")\n#                 pass\n\n#     pool.close()\n#     pool.join()\n\n#     print(f\"Successfully converted {total_success} chunks out of {total_chunks} total\")\n\n\n# process_data_multiprocessing(train_df, 'train', num_processes=8) # Example: Use 8 processes\n# process_data_multiprocessing(test_df, 'test', num_processes=8) # Example: Use 8 processes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nimport shutil\n\ndef zip_download_folder(download_folder_path, output_zip_path):\n    try:\n        # Check if the download folder exists\n        if not os.path.exists(download_folder_path):\n            raise FileNotFoundError(f\"Download folder not found: {download_folder_path}\")\n\n        # Create a ZipFile object in write mode\n        with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:  # ZIP_DEFLATED for compression\n            for root, _, files in os.walk(download_folder_path): # Walk through all subdirectories\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    arc_name = os.path.relpath(file_path, download_folder_path)  # Get relative path for archive\n                    zipf.write(file_path, arc_name)  # Write the file to the zip archive\n\n        print(f\"Download folder zipped successfully to: {output_zip_path}\")\n\n    except FileNotFoundError as e:\n        print(f\"Error: {e}\")\n    except Exception as e:\n        print(f\"An error occurred during zipping: {e}\")\n\n\n# # Example usage:\n# download_folder = \"/kaggle/working/test\"  # Replace with the actual path to your download folder\n# output_zip = \"/kaggle/working/test.zip\"  # Replace with the desired output zip file path\n\n# zip_download_folder(download_folder, output_zip)\n\n# download_folder = \"/kaggle/working/train\"  # Replace with the actual path to your download folder\n# output_zip = \"/kaggle/working/train.zip\"  # Replace with the desired output zip file path\n\n# zip_download_folder(download_folder, output_zip)\n\ndownload_folder = \"/kaggle/working/sorted_data\"  # Replace with the actual path to your download folder\noutput_zip = \"/kaggle/working/sorted_data.zip\"  # Replace with the desired output zip file path\n\nzip_download_folder(download_folder, output_zip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T09:00:13.767785Z","iopub.execute_input":"2025-02-08T09:00:13.768104Z","iopub.status.idle":"2025-02-08T09:01:22.366887Z","shell.execute_reply.started":"2025-02-08T09:00:13.768080Z","shell.execute_reply":"2025-02-08T09:01:22.365966Z"}},"outputs":[{"name":"stdout","text":"Download folder zipped successfully to: /kaggle/working/sorted_data.zip\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink('/kaggle/working/sorted_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# train","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ndata_dir = \"/kaggle/input/signal-fast-radio-burst-detection/train-labels-corrected/train\"\n\ndfs = []\n\n# Loop through files in the directory\nfor idx, file in enumerate(os.listdir(data_dir)):  # Use enumerate to get an index\n    if file.endswith(\".csv\"):  # Check if file is a CSV\n        # print(file)\n        # break\n        file_path = os.path.join(data_dir, file)\n        df = pd.read_csv(file_path)\n        # print(file)\n        # break\n        file_name = file.strip(\"labels.csv\")\n        # Create a name column with \"file_index_rowindex\" format\n        df[\"name\"] = [f\"{file_name}{i}.jpg\" for i in range(len(df))]\n\n        dfs.append(df)\n\n# Merge all dataframes\nmerged_df = pd.concat(dfs, ignore_index=True)\nmerged_df.drop(columns =[\"Unnamed: 0\",\"index\"],inplace =True)\n# Save the merged dataframe to a new CSV file\nmerged_df.to_csv(\"merged.csv\", index=False)\n\nprint(\"All CSV files have been merged successfully into merged.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:41:56.631166Z","iopub.execute_input":"2025-02-08T08:41:56.631487Z","iopub.status.idle":"2025-02-08T08:41:57.678317Z","shell.execute_reply.started":"2025-02-08T08:41:56.631462Z","shell.execute_reply":"2025-02-08T08:41:57.677491Z"}},"outputs":[{"name":"stdout","text":"All CSV files have been merged successfully into merged.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"merged.csv\")\ndf.fillna(\"NAN\",inplace =True)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:42:03.563335Z","iopub.execute_input":"2025-02-08T08:42:03.563621Z","iopub.status.idle":"2025-02-08T08:42:03.679097Z","shell.execute_reply.started":"2025-02-08T08:42:03.563601Z","shell.execute_reply":"2025-02-08T08:42:03.678355Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"       labels                                              name\n0         NAN     B1933+16_59475_68410_reduced_fc_0001023_0.jpg\n1         NAN     B1933+16_59475_68410_reduced_fc_0001023_1.jpg\n2         NAN     B1933+16_59475_68410_reduced_fc_0001023_2.jpg\n3         NAN     B1933+16_59475_68410_reduced_fc_0001023_3.jpg\n4         NAN     B1933+16_59475_68410_reduced_fc_0001023_4.jpg\n...       ...                                               ...\n96420  Narrow  B0531+21_58713_43190_reduced_fc_0007167_1019.jpg\n96421  Narrow  B0531+21_58713_43190_reduced_fc_0007167_1020.jpg\n96422  Narrow  B0531+21_58713_43190_reduced_fc_0007167_1021.jpg\n96423  Narrow  B0531+21_58713_43190_reduced_fc_0007167_1022.jpg\n96424  Narrow  B0531+21_58713_43190_reduced_fc_0007167_1023.jpg\n\n[96425 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NAN</td>\n      <td>B1933+16_59475_68410_reduced_fc_0001023_0.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NAN</td>\n      <td>B1933+16_59475_68410_reduced_fc_0001023_1.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NAN</td>\n      <td>B1933+16_59475_68410_reduced_fc_0001023_2.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NAN</td>\n      <td>B1933+16_59475_68410_reduced_fc_0001023_3.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NAN</td>\n      <td>B1933+16_59475_68410_reduced_fc_0001023_4.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>96420</th>\n      <td>Narrow</td>\n      <td>B0531+21_58713_43190_reduced_fc_0007167_1019.jpg</td>\n    </tr>\n    <tr>\n      <th>96421</th>\n      <td>Narrow</td>\n      <td>B0531+21_58713_43190_reduced_fc_0007167_1020.jpg</td>\n    </tr>\n    <tr>\n      <th>96422</th>\n      <td>Narrow</td>\n      <td>B0531+21_58713_43190_reduced_fc_0007167_1021.jpg</td>\n    </tr>\n    <tr>\n      <th>96423</th>\n      <td>Narrow</td>\n      <td>B0531+21_58713_43190_reduced_fc_0007167_1022.jpg</td>\n    </tr>\n    <tr>\n      <th>96424</th>\n      <td>Narrow</td>\n      <td>B0531+21_58713_43190_reduced_fc_0007167_1023.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>96425 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# df = df[~df.labels.isin(['Unlabeled','Uncertain'])]\nfor l in df.labels.unique():\n    print(len(df[df.labels == l]),l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:42:56.711087Z","iopub.execute_input":"2025-02-08T08:42:56.711393Z","iopub.status.idle":"2025-02-08T08:42:56.791753Z","shell.execute_reply.started":"2025-02-08T08:42:56.711372Z","shell.execute_reply":"2025-02-08T08:42:56.790910Z"}},"outputs":[{"name":"stdout","text":"62988 NAN\n3847 Pulse\n3201 Broad\n9 Broad+Pulse\n11842 Unlabeled\n5721 Uncertain\n8582 Narrow\n60 Narrow+Pulse\n138 Narrow+Broad\n37 Unknown+Pulse\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\nX = df.name\ny = df.labels\n# X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:43:54.150963Z","iopub.execute_input":"2025-02-08T08:43:54.151306Z","iopub.status.idle":"2025-02-08T08:43:54.155363Z","shell.execute_reply.started":"2025-02-08T08:43:54.151282Z","shell.execute_reply":"2025-02-08T08:43:54.154578Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming you have X (features) and y (labels) variables already defined\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training data shape:\", X_train.shape)\nprint(\"Test data shape:\", X_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:44:04.524247Z","iopub.execute_input":"2025-02-08T08:44:04.524534Z","iopub.status.idle":"2025-02-08T08:44:05.077939Z","shell.execute_reply.started":"2025-02-08T08:44:04.524514Z","shell.execute_reply":"2025-02-08T08:44:05.077213Z"}},"outputs":[{"name":"stdout","text":"Training data shape: (77140,)\nTest data shape: (19285,)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\ndef makeDataset(X,y , folder_name = \"train\"):\n\n    df = pd.concat([X, y], axis=1)\n    # Define paths\n    data_dir = \"/kaggle/input/preprocess/train (1)\"  # Source directory\n    output_dir = f\"/kaggle/working/sorted_data/{folder_name}\"  # Destination directory\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    \n    # Iterate over unique labels and copy files\n    for label in y.unique():\n        label_dir = os.path.join(output_dir, str(label))  # Create folder for label\n        os.makedirs(label_dir, exist_ok=True)\n    \n        # Select rows with the current label\n        x_df = df[df[\"labels\"] == label]\n    \n        # Copy files into respective label folders\n        for _, row in x_df.iterrows():\n            file_name = row[\"name\"]  # Assuming this is the filename\n            src_path = os.path.join(data_dir, file_name)  # Source file path\n            dst_path = os.path.join(label_dir, file_name)  # Destination path\n            \n            if os.path.exists(src_path):  # Ensure file exists before copying\n                shutil.copy(src_path, dst_path)\n    \n    print(\"Files successfully copied into their respective label folders!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:44:21.796054Z","iopub.execute_input":"2025-02-08T08:44:21.796501Z","iopub.status.idle":"2025-02-08T08:44:21.802526Z","shell.execute_reply.started":"2025-02-08T08:44:21.796477Z","shell.execute_reply":"2025-02-08T08:44:21.801704Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"makeDataset(X_train,y_train,'train')\nmakeDataset(X_test,y_test,'val')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:44:28.173423Z","iopub.execute_input":"2025-02-08T08:44:28.173709Z","iopub.status.idle":"2025-02-08T08:52:44.949681Z","shell.execute_reply.started":"2025-02-08T08:44:28.173687Z","shell.execute_reply":"2025-02-08T08:52:44.948862Z"}},"outputs":[{"name":"stdout","text":"Files successfully copied into their respective label folders!\nFiles successfully copied into their respective label folders!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# import shutil\n# import os\n\n# # Define the folder path\n# folder_path = \"/kaggle/working/sorted_data\"\n\n# # Check if the folder exists before attempting to delete\n# if os.path.exists(folder_path) and os.path.isdir(folder_path):\n#     shutil.rmtree(folder_path)\n#     print(f\"Folder {folder_path} and all its contents have been deleted.\")\n# else:\n#     print(f\"The folder {folder_path} does not exist.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T09:17:09.557453Z","iopub.execute_input":"2025-02-08T09:17:09.557738Z","iopub.status.idle":"2025-02-08T09:17:14.900293Z","shell.execute_reply.started":"2025-02-08T09:17:09.557718Z","shell.execute_reply":"2025-02-08T09:17:14.899185Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.73-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.73-py3-none-any.whl (914 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.6/914.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.3.73 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a YOLOv8 model pretrained on ImageNet\nmodel = YOLO(\"yolo11l-cls\")  # Use \"yolov8s-cls.pt\" for a larger model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T09:17:20.572491Z","iopub.execute_input":"2025-02-08T09:17:20.572833Z","iopub.status.idle":"2025-02-08T09:17:27.519388Z","shell.execute_reply.started":"2025-02-08T09:17:20.572801Z","shell.execute_reply":"2025-02-08T09:17:27.518498Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt to 'yolo11l-cls.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 27.2M/27.2M [00:00<00:00, 253MB/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model.train(data=\"/kaggle/working/sorted_data\", epochs=10, imgsz=224)\n# # Run inference on an image\n# results = model(\"example.jpg\")  # Replace with your image path\n\n# # Print the predicted class\n# print(results)|\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T09:17:28.608998Z","iopub.execute_input":"2025-02-08T09:17:28.609577Z","iopub.status.idle":"2025-02-08T09:26:44.217005Z","shell.execute_reply.started":"2025-02-08T09:17:28.609535Z","shell.execute_reply":"2025-02-08T09:26:44.215608Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.73 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11l-cls.pt, data=/kaggle/working/sorted_data, epochs=10, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n\u001b[34m\u001b[1mtrain:\u001b[0m /kaggle/working/sorted_data/train... found 77140 images in 10 classes ✅ \n\u001b[34m\u001b[1mval:\u001b[0m /kaggle/working/sorted_data/val... found 19285 images in 10 classes ✅ \n\u001b[34m\u001b[1mtest:\u001b[0m None...\nOverriding model.yaml nc=80 with nc=10\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n  9                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n 10                  -1  1    670730  ultralytics.nn.modules.head.Classify         [512, 10]                     \nYOLO11l-cls summary: 309 layers, 12,847,434 parameters, 12,847,434 gradients, 49.8 GFLOPs\nTransferred 492/494 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5.35M/5.35M [00:00<00:00, 106MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/sorted_data/train... 77140 images, 0 corrupt: 100%|██████████| 77140/77140 [00:35<00:00, 2159.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/sorted_data/train.cache\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/sorted_data/val... 19285 images, 0 corrupt: 100%|██████████| 19285/19285 [00:08<00:00, 2160.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/sorted_data/val.cache\n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 82 weight(decay=0.0), 83 weight(decay=0.0005), 83 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 224 train, 224 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/classify/train\u001b[0m\nStarting training for 10 epochs...\n\n      Epoch    GPU_mem       loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      1.06G      2.265         16        224:   0%|          | 4/4822 [00:01<15:52,  5.06it/s]","output_type":"stream"},{"name":"stdout","text":"Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"       1/10      1.13G      2.295         16        224:   0%|          | 9/4822 [00:01<09:35,  8.37it/s]\n100%|██████████| 755k/755k [00:00<00:00, 25.9MB/s]\n       1/10      1.15G     0.8482          4        224: 100%|██████████| 4822/4822 [07:32<00:00, 10.66it/s]\n               classes   top1_acc   top5_acc: 100%|██████████| 603/603 [00:35<00:00, 17.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all      0.751      0.999\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-06005daa84ee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/working/sorted_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# # Run inference on an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# results = model(\"example.jpg\")  # Replace with your image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # Print the predicted class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m  \u001b[0;31m# do not move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m  \u001b[0;31m# stop if exceeded epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_fit_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mrun_callbacks\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;34m\"\"\"Run all existing callbacks associated with a particular event.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/callbacks/raytune.py\u001b[0m in \u001b[0;36mon_fit_epoch_end\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mon_fit_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\"Sends training metrics to Ray Tune at end of each epoch.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# replacement for deprecated ray.tune.is_session_enabled()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'ray.train._internal.session' has no attribute '_get_session'"],"ename":"AttributeError","evalue":"module 'ray.train._internal.session' has no attribute '_get_session'","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}